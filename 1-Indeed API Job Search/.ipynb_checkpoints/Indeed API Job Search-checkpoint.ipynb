{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Python 2.7 #############\n",
    "import urllib\n",
    "import requests, json\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pymongo import MongoClient \n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "class IndeedCrawl(object):\n",
    "\n",
    "    def __init__(self,data_table):\n",
    "        #self.query = \"jobtitle:({0})\".format(company_name)\n",
    "        \n",
    "        self.endpoint = 'http://api.indeed.com/ads/apisearch'\n",
    "        self.key = \"6437595457989494\"\n",
    "        self.client = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "        self.db=self.client.jobs[data_table]\n",
    "\n",
    "    def _create_params(self, query, start, location, job_type ,count=3):\n",
    "        params = urllib.urlencode({\n",
    "            # Request parameters\n",
    "            'publisher': self.key,\n",
    "            'q': query,\n",
    "            'limit': count,\n",
    "            'co': \"US\",\n",
    "            'start': start,\n",
    "            'sort': 'date',\n",
    "            'st': 'employer',\n",
    "            'format': 'json',\n",
    "            'v': 2,\n",
    "            'l': location,\n",
    "            'filter':1,\n",
    "            'jt': job_type\n",
    "        })\n",
    "        return params\n",
    "\n",
    "    def search(self, query, start, location, job_type,count=10):\n",
    "        jobs = []\n",
    "        try:\n",
    "            while not start > count:\n",
    "                params = self._create_params(query, start, location,job_type, count)\n",
    "                response = requests.get(self.endpoint, params)\n",
    "                response = json.loads(response.text)\n",
    "                count = response['totalResults']\n",
    "                print \"start= \", start\n",
    "                print count\n",
    "\n",
    "                for res in response['results']:\n",
    "                    if count == 0:\n",
    "                        break\n",
    "                    if res.get('url'):\n",
    "                        url = res.get('url')\n",
    "                        res2 = requests.get(url)\n",
    "                        jd = \"\"\n",
    "                        summary = lxml.html.fromstring(res2.text)\n",
    "                        for span in summary.xpath('//span[@id=\"job_summary\"]'):\n",
    "                            for x in span.itertext():\n",
    "                                #job_sum = x.encode('utf-8')\n",
    "                                job_sum=x\n",
    "                                jd += job_sum\n",
    "                        res['job_summary'] = jd\n",
    "                        self.db.insert(res)\n",
    "                    print \" job: \", res['jobtitle'], \"\\n\"\n",
    "                    jobs.append(res)\n",
    "                print \"start query again\"\n",
    "                start += 25\n",
    "                #count -= 25\n",
    "                \n",
    "            return jobs\n",
    "        except Exception as e:\n",
    "            print e.message\n",
    "    \n",
    "    def output_csv(self,filename):\n",
    "\n",
    "        data = self.db.find({})\n",
    "        data=list(data)\n",
    "        data=pd.DataFrame(data)\n",
    "        data=data.drop_duplicates(\"jobkey\")\n",
    "        data.to_csv(str(filename)+\".csv\",encoding=\"utf-8\")\n",
    "    \n",
    "    def output_excel(self,filename):\n",
    "\n",
    "        data = self.db.find({})\n",
    "        data=list(data)\n",
    "        data=pd.DataFrame(data)\n",
    "        data=data.drop_duplicates(\"jobkey\")\n",
    "        data=data.drop(\"_id\",1)\n",
    "        data.to_excel(str(filename)+\".xlsx\",encoding=\"utf-8\")\n",
    "\n",
    "####################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term=\"data\"\n",
    "location=\"95691\"\n",
    "data_table=\"indeed_sacramento_research\"\n",
    "job_type=\"parttime\" #internship, parttime, fulltime\n",
    "result_count=500\n",
    "my_test=IndeedCrawl(data_table)\n",
    "result = my_test.search(search_term,0,location,job_type,result_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_test.output_excel(\"parttime-sacramento-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test.db.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test.db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
